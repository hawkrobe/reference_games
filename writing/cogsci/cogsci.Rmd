---
title: "Convention-formation in a communication model"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert Hawkins, Michael Frank, Noah Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology \\ Stanford University}
 
abstract: 
    "What cognitive mechanisms support the emergence of conventions in repeated interactions? We argue that conventions form when agents assume they already exist and attempt to learn them through reasoning about knowledgeable, informative partners. We formalize this theory in a computational model of language understanding as social inference and demonstrate that it explains several key empirical signatures of conventionalization in language games including arbitrariness, path-dependence, and reduction in utterance length over time. In particular, we present results from a large-scale, multi-player replication of a classic *tangrams* paradigm which show that reduction is primarily due to the speaker dropping meaningful modifiers and clauses. According to our model, these additional sources of information are initially useful in hedging against uncertainty about the conventional lexicon but become redundant as the agent learns."
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---



```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
library(devtools)
library(cogsci2016)
```

```{r, libraries, include = FALSE}
library(ggplot2)
library(lme4)
library(entropy)
library(tm)
library(tidyr)
library(dplyr)
library(stringr)
library(knitr)
library(readr)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(rwebppl)
```

# Introduction

Just as drivers depend on shared behavioral conventions to safely navigate traffic, successful communication depends on a set of shared linguistic conventions. Speakers of different languages around the world refer to the same object in many different ways, yet when ordering a coffee in San Francisco, I can confidently use the English word "coffee" and assume that I will be understood. How do these conventions -- classically characterized by @Lewis69_Convention as arbitrary but stable solutions to recurring coordination problems -- form in the first place? 


While *global* conventions adopted and sustained throughout a large population of speakers may develop over longer time scales [@Campbell13_HistoricalLinguistics], we also effortlessly coordinate on *local* conventions -- or conceptual pacts [@BrennanClark96_ConceptualPactsConversation] -- within the span of a single dialogue. For example, when discussing possible conditions to use in an upcoming experiment, a team of collaborators might begin the meeting using long descriptions to refer to each condition but end the meeting using conventional terms like "condition A" and "condition B". Since global conventions are hypothesized to emerge through diffuse, repeated interactions of this kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying convention-formation in such games are of foundational interest.


In a seminal study by @KraussWeinheimer64_ReferencePhrases, pairs of participants played an cooperative language game where they were presented with arrays of ambiguous shapes in randomized orders. The players were assigned the roles of *director* and *matcher* and allowed to talk freely. The matcher's goal was to rearrange their shapes to match the director's board, and the director's goal was to communicate useful descriptions. Over multiple rounds, descriptions were dramatically shortened: an early description like "upside-down martini glass in a wire stand," became simply "martini" by the end. @ClarkWilkesGibbs86_ReferringCollaborative later refined this paradigm to use larger arrays of tangram-like figures. They not only replicated the reduction of utterance length over time, but also emphasized the intricate collaborative process through which speakers and listeners negotiate over references.

These studies reveal a number of key empirical signatures constraining cognitive mechanisms of convention-formation, which differ primarily in the extent to which sophisticated social reasoning and common ground is required. At one extreme, agents use simple heuristic updating rules and do not need to represent or reason about other agents at all  [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems]. At the other extreme, agents explicitly track what information is held in *mutual knowledge* and choose actions based on the expected beliefs of other agents [@WilkesGibbsClark92_CoordinatingBeliefs;@Lewis69_Convention].

In this paper, we propose a theoretical position on the spectrum between these poles: conventions form when agents *assume conventions already exist* and attempt to learn them by reasoning about a knowledgeable, informative partner. First, we conduct a finer-grained analysis of reduction in a large-scale, multi-player replication of the tangrams task, showing that the primary source of reduction is the dropping of meaningful clauses. Next, we formulate this theory in a computational model of communication in repeated reference games, based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS]. Finally, we show that this model captures key empirical signatures of convention-formation, including arbitrariness, path-dependence, and systematic shortening of utterances over time. 

```{r image, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=3.5, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial in experimental interface. Both players could freely use the chat box, and the matcher could click and drag the tangram images."}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

# Large-scale tangrams replication

## Methods

```{r}
# Import message data...
tangramMsgs = read_csv("../../analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

### Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded \textcolor{red}{X} games because one or both of the participants reported being a native language different from English, and \textcolor{red}{Y} games for terminating early, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances. \textcolor{red}{rdh: this technically includes nicki's data, too; need to make this clearer}

### Stimuli

On every trial of the game, both participants were shown a grid of twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. The cells were labeled with numbers from one to twelve, as an aid in reference (see Fig. \ref{fig:taskScreenshot}).

### Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time, but only the matcher could click and drag stimuli to reorder them. When the players were satisfied with their tangram arrangements, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent and every swapping action taken by the matcher. 

## Results 

<!-- ### Preprocessing  -->

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

<!-- \textcolor{red}{rdh: as Mike points out, I think this was a valient but ultimately doomed attempt... We should either run the sequential version of the task, pay turkers to label the rest, or stick to analyses that don't require these tags (gotta leave something for the journal paper...)} -->

<!-- Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus).  -->

<!-- We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag. -->

### Reduction in raw length

```{r replicationfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:replication} Reduction in the mean length of utterance over time. Error bars represent 1 SE."}
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words per tangram") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw(8) 
```

First, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation], although perhaps due to our purely textual (vs. spoken) interface, participants in our task used much fewer words overall. The following analyses break down this broad reduction into a finer-grained set of phenomena. 

### Listener feedback aids conventionalization

```{r turntakingfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:turntaking} Ratio of messages sent by speaker relative to listener. Error bars represent 1 SE."}
listenerMsgs <- tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) 

listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))

ggplot(listenerMsgs %>%    
         group_by(roundNum) %>% 
         summarize(m = mean(matcher),
             se = sd(matcher)/sqrt(length(matcher))), aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("# listener messages") +
  xlab("trials") +
  #ylim(.5,1) +
  xlim(0, 7) +
  theme_bw(8) 

```

```{r}
turnTaking <- listenerMsgs %>% 
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>% 
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>% 
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw() +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
```


The theory proposed by @ClarkWilkesGibbs86_ReferringCollaborative argues that lexical conventions are established through a collaborative process requiring both speaker and listener input. This predicts that (1) listener feedback should be highest on the first round and drop off once meanings are agreed upon, and (2) dyads with more initial listener feedback should converge on more efficient conventions. We find both of these patterns in our data. The number of listener messages decreases significantly over the game ($t = -13.23$, see Fig. \ref{fig:turntaking}), and there is a weak but significant effect of initial listener messages on overall reduction (`r turnTakingResult`).

### Part-of-speech reduction

```{r POSfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:POS} Percent reduction between first and last round for closed-class and open-class parts of speech over time. Error bars represent 1 SE."}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  # Take mean & se over participants
  summarize(diffPctM = mean(diffPct),
            diffPctSE = sd(diffPct)/sqrt(length(diffPct))) %>%
  filter(POS != "OTHER") %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -diffPctM) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$diffPctM * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$diffPctM * 100

ggplot(posReduction, aes(x = POS, y = diffPctM, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = diffPctM + diffPctSE, ymin = diffPctM - diffPctSE), width = .1)+
  theme_bw(8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

What kinds of words are most likely to be dropped as the game proceeds? We used the Stanford CoreNLP part-of-speech tagger [@Toutanova03_POStagging] to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:POS} shows the percent reduction of different parts of speech from the first round to the sixth round. We find that determiners ('the', 'a', 'an') are the most likely class of words to be dropped with an `r round(detReductionRate)`% reduction rate, on average. Nouns ('dancer', 'rabbit') are the least likely class to be dropped with only an `r round(nounReductionRate)`% rate. Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech. 

### Reduction in meaningful clauses

```{r xtable, results="asis", caption="fasdfasdf"}
unigrams <- read_csv("../../analysis/tangrams/wordCounts.csv", col_names = T) %>%
  group_by(word, POS, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>%
  select(word) %>% 
  filter(word != ',') %>%
  filter(word != '#') %>%
  head(n = 10)

bigrams <- read_csv("../../analysis/tangrams/bigramCounts.csv", col_names = T) %>%
  group_by(word, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>% 
  select(word) %>% 
  rename(bigrams = word) %>%
  head(n = 10)

topWords <- t(cbind(unigrams, bigrams))
colnames(topWords) <- NULL
rownames(topWords) <- c('unigrams', 'bigrams')
print(xtable(topWords, label = 'tab:words', caption = 'Top 10 unigrams and bigrams with the highest reduction'), 
      floating.environment = "table*", comment=F, table.placement = 'th')
```

However, examining the words most likely to be dropped (see Table \ref{tab:words}), we notice that alongside dropped articles, there are a number of words that form conjunctions ('and') and modifying clauses ('of', 'with', 'the right'). This suggests that when utterances are reduced, it is not merely formal function words that are dropped from laziness. Initial phrases pile on multiple ambiguous but redundant modifiers and descriptors: as the game progresses and ambiguity of reference decreases, these additional meaningful units become less useful and can be dropped. We explicitly examined this hypothesis by running a dependency parser and tagging the occurence of dependent clauses and modifiers. 

\textcolor{red}{to-do: use dependency parser to count dependent clauses and modifiers...}

### Arbitrariness and path-dependence

```{r entropy}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  #cleanedCorpus <- tm_map(corpus, removeWords, stopwords('english'))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

# TODO: double-check that we're controlling for size of word distribution 
# (right now might slightly misleading just because support of acrossPair distribution is much bigger...)
withinPair <- tangramCombined %>% 
  group_by(gameid) %>%
  mutate(alphabetSize = length(getCounts(contents)),
         ent = entropy(getCounts(contents))) %>%
  ungroup() %>%
  #summarize(normedEnt = mean(ent)/log(mean(alphabetSize))) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent)))

acrossPair <- tangramCombined %>% 
  group_by(roundNum) %>% 
  mutate(alphabetSize = length(getCounts(contents)),
         ent = entropy(getCounts(contents))) %>%
  #summarize(normedEnt = mean(ent)/log(mean(alphabetSize))) %>%
  ungroup() %>%
  summarize(acrossEnt = mean(ent))
```

A final key signature of conventions is their *arbitrariness* [@Lewis69_Convention]: different pairs may converge on different but equally successful referring expressions. By the final round, for example, one pair successfully communicated about one tangram as the 'dancer' while another successfully used 'ice skater' to refer to the same object. However, once a pair adopts a convention, it is usually in their best interest to continue using it: in this sense, conventions are *path-dependent*. In our data, we can operationalize these concepts by comparing the entropy of the word distribution *across* games against the entropy *within* games. In the absence of arbitrariness, we would expect entropy to be lower across different games, since all players would adopt the single optimal way to refer to each tangram. Similarly, in the absence of path-dependence, we would expect entropy to be higher within games, since participants would be randomly sampling arbitrary referents each round. The across-game entropy for a given round $r_i$ is defined by
$$H(r_i) = \sum_{w \in \mathcal{L}_{r_i}} P(w) \log P(w)$$
where $P(w)$ is the empirical word-frequency distribution defined on the set of unique words $\mathcal{L}_i$ found across all utterances produced in all games in round $r_i$. The within-game entropy for a given game $g_i$ is given by substituting in the distribution $P(w)$ over the unique words $\mathcal{L}_{g_i}$ produced in that game. We find a mean entropy of `r round(withinPair$withinEnt,2)` within games, which is smaller than the mean entropy of `r round(acrossPair$acrossEnt, 2)` across games. In other words, there is more variability in the expressions chosen across different speakers than there is across repeated interactions of a single speaker. This pattern, though indirect, is consistent with both signatures of arbitrariness and path-dependence. \textcolor{red}{rdh: mention why I'm not normalizing entropies by alphabet size? (at first I thought this was right, but changed my mind: it's precisely the increase in total number of words we're interested in...}

# Model

Here, we present a probabilistic model of language production under uncertainty, which captures several of the signiture properties of conventions shown above. This model belongs to the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena -- including scalar implicature, adjectival vagueness, overinformativeness, indirect questions, and non-literal language use -- as arising from a process of recursive social reasoning. Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a question of speaker production. An $n$th order pragmatic speaker trying to convey a particular state of affairs $s \in \mathcal{S}$ assuming lexicon $\mathcal{L}$ is assumed to select an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length [@GoodmanFrank16_RSATiCS]:

$$S_n(u | s, \mathcal{L}) \propto \exp{\left(\alpha \log L_n(s | u, \mathcal{L}) - \textrm{cost}(u)\right)}$$

where $\alpha$ is an optimality parameter controlling the extent to which the speaker maximizes over the expected listener distribution. The listener, in turn, reasons about what utterances would be most likely to be produced by a speaker intending to convey $u$:

$$L_n(s | u, \mathcal{L}) \propto P(s) S_{n-1}(u | s, \mathcal{L})$$

This recursion bottoms out in a *literal listener* who directly looks up the meaning of the utterance in the lexicon:

$$L_0(s | u, \mathcal{L}) \propto \mathcal{L}(u, s)\cdot P(s)$$

As in several other recent applications of RSA [@GrafEtAl16_BasicLevel], we use a graded semantics, where utterances are better or worse descriptions of particular referents. For instance, the utterance "dancer" may initially be expected to apply to a photorealistic image of a ballerina ($\mathcal{L}(\textrm{'dancer'}, \textrm{*ballerina*}) = 0.99$) more than an abstract image of one ($0.6$), but apply to both better than a non-category member like an image of a dog ($0.05$).

Our approach to convention-formation begins with the additional assumption of *lexical uncertainty* [@SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS;@BergenLevyGoodman16_LexicalUncertainty]. In other words, we assume that instead of having perfect knowledge of $\mathcal{L}$, the speaker has uncertainty over the exact meanings of lexical items in the current context (i.e. it is initially unclear which of the ambiguous tangram shapes "the dancer" might refer to). They begin with some prior $P(\mathcal{L})$ over meanings, which may be initially biased toward one certain meanings, and updates these beliefs through repeated interactions with a knowledgeable partner. 

$$P(\mathcal{L} | d) = P(\mathcal{L})\prod_i L_{n-1}(s_i|u_i, \mathcal{L})$$

where $d = \{s_i, u_i\}$ is a set of observations of $s_i$ and $u_i$ coming from previous exchanges. \textcolor{red}{TODO: this notation makes the subscripts when describing the space of states/utterances confusing}. The speaker then marginalizes over this posterior distribution when reasoning about the listener, giving rise to the form of the pragmatic listener model we use throughout our model results (only going up to $n = 2$ in our recusion for simplicity):

$$S(u | s, d) = \exp( \alpha\log\left(\sum_{\mathcal{L}} P(\mathcal{L} | d) L_1(s | u, \mathcal{L})\right) - \textrm{cost}(u) )$$

A listener with lexical uncertainty can be defined similarity, simply swapping out $L_{n-1}$ in the lexicon posterior update with a knowledgeable speaker $S_{n-1}$:

$$L(s | u, d) = \sum_\mathcal{L}P(\mathcal{L}|d)L_1(s|u,\mathcal{L})$$

This model is implemented in the probabilistic programming language WebPPL [@GoodmanStuhlmuller14_DIPPL]\footnote{All results can be reproduced running our code in the browser at http://forestdb.org/models/conventions.html}. Following  @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS, we begin by showing how a random initial choice is taken to be evidence for a particular lexicon and becomes the base for successful communication even though neither party knows its meaning at the outset.

## Arbitrariness

```{r model1, cache=TRUE, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelAcc} Accuracy rises as speaker and listener learn from the same data."}
res <- webppl(program_file = 'webpplModels/arbitrary.wppl',
             model_var = 'model',
             inference_opts = list(method="forward", samples=5000),
             data = 6,
             data_var = 'numSteps',
             output_format = 'samples')
accuracyRes <- res %>% 
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  select(-garbage) %>%
  spread(type, val) %>%
  group_by(roundNum, acc) %>%
  tally() %>%   
  mutate(prob = n / sum(n)) %>%
  ungroup() %>%
  filter(acc == TRUE) %>%
  mutate(roundNum = as.numeric(roundNum)) 
ggplot(accuracyRes, aes(x = roundNum, y = prob)) +
  ylab('accuracy') +
  geom_line() +
  theme_bw()
```

```{r model_arbitrariness, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelConvergence} Accuracy rises as speaker and listener learn from the same data."}
accuracyRes <- res %>%
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  select(-garbage) %>%
  spread(type, val) %>%
  unite(intendedPair, intended, utt, sep = '<->', remove = F) %>% 
  unite(realPair, utt, response, sep = '<->', remove = F) %>% 
  group_by(sampleId) %>% 
  mutate(firstPair = first(realPair)) %>%
  group_by(firstPair, roundNum, intendedPair) %>% 
  tally() %>%
  ungroup() %>%
  complete(firstPair, roundNum, intendedPair, fill = list(n = 0)) %>%  
  group_by(firstPair, roundNum) %>%
  mutate(prob = n/sum(n)) %>%
  filter(intendedPair == 't1<->label1') %>%
  filter(firstPair %in% c('label1<->t1', 'label1<->t2'))

ggplot(accuracyRes, aes(x = roundNum, y = prob, 
                        group = firstPair, linetype =firstPair, color = firstPair)) +
  ylab("S(label1 | t1) (expected)") +
  geom_line() +
  theme_bw(8) +
  theme( plot.margin=unit(x=c(0,0,0,0),units="mm"), legend.position="top")
```

Consider an environment with two abstract shapes ($\{s_1, s_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) incurring equal cost. Their prior $P(\mathcal{L})$ over the meaning of each utterance is given by a (discretized) Dirichlet distribution, so on the first round both utterances are equally likely to apply to either shape. If the speaker was trying to get their partner to pick $s_1$, then, since each utterance is equally (un)informative, they would randomly sample one (say, $u_1$), and observe the listener's selection of a shape (say, $s_1$). On the next round, the speaker uses the observed pair $\{u_1, s_1\}$ to update their beliefs about the lexicon, uses these beliefs to generate a new utterance, and so on. To examine expected dynamics over multiple rounds, we enumerate over all possible trajectories our simulated speaker and listener models could produce.

We observe several important qualitative effects in our simulations. First, the fact that a knowledgeable listener responds to utterance $u$ with $s$ provides evidence for lexicons in which $u$ is a good fit for $s$, hence the likelihood of the speaker using $u$ to refer to $s$ increases on subsequent rounds (see Fig.\ref{fig:modelConvergence}). In other words, the initial symmetry between the meanings can be broken by initial random choices, leading to completely *arbitrary but stable mappings* in future rounds. Second, because the listener is also learning the lexicon from these observations under the same set of assumptions, they converge on a shared set of meanings; hence, expected *accuracy* rises on future rounds (see Fig. \ref{fig:modelAcc}). Third, because one's partner is assumed to be pragmatic, agents can also learn about *unheard* utterances: observing $\{u_1, s_1\}$ also provides evidence for lexicons in which $u_2$ is a good fit for $s_2$ by standard Gricean reasoning. Finally, *failed references* lead to conventions just as effectively as successful references: if the speaker intends $s_1$ and says $u_1$, but then the listener incorrectly picks $s_2$, the speaker will take this as evidence that $u_1$ actually means $s_1$ and use it that way on subsequent rounds.

\textcolor{red}{possible TODOs: split out model curves by diff alphas (currently shown for alpha = 5)? Collapse across 'conventional systems', e.g. treat t1:label1/t2:label2 as one system and t1:label2/t2:label1 as another?}

## Reduction of modifiers \& conjunctions

```{r modelReduction, cache=TRUE}
conjunctionRes <- webppl(program_file = 'webpplModels/conjunction.wppl',
             model_var = 'model',
             inference_opts = list(method="forward", samples=1000),
             data = list(numSteps=6, alpha=13, beta=1),
             data_var = 'params', 
             output_format = 'samples')
```

```{r modelReductionPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelReduction} Utterances tend to get shorter over time"}
reductionRes <- conjunctionRes %>% 
  select(value.utt.0:value.acc.3) %>%
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  spread(type, val) %>%
  mutate(numWords = str_count(utt, "\\S+")) %>%
  group_by(roundNum, garbage) %>%
  summarize(m = mean(numWords), 
            se = sd(numWords)/sqrt(length(numWords)))

ggplot(reductionRes, aes(x = roundNum, y = m, group = garbage)) +
  ylab('mean utterance length') +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +  
  theme_bw()
```

Next, we show that by (1) adding a small initial bias on the meanings of utterances, and (2) extending our grammar to include conjunctions of lexical items, it is natural for a speaker to produce a (redundant) conjunction on the first trial and to drop one of the constituents shortly after (see \ref{fig:modelReduction}).

\textcolor{red}{TODO: set-up this scenario better}

We see that thereâ€™s an initial preference for the longer conjunction of "type_a" and "color_a" despite the utterance cost because

(1) there's an initial bias in the lexicon prior for 'a' utterances to correspond to properties with the value 0 (thus why neither of the 'b' utterances are assigned any probability)

(2) the conjunction hedges against unlikely but possible lexicons where one or the other utterance actually corresponds to a property with value 1.

After observing an example of this conjunction referring to the first state, the conjunction actually becomes more preferred by the speaker, as it increases the probability of utterances where both type_a and color_a mean the first state. The evidence is indeterminate about the separate meanings of "type_a" and "color_a". Because of cost considerations, however, the shorter utterances are still assigned some probability. Once the speaker produces one or the other short utterances by chance, then, it breaks the symmetry and this shorter utterance becomes the most probable in future rounds.

# General Discussion

### Discuss what this says about arbitrariness?
A critical aspect of social conventions, and linguistic conventions in particular, is their arbitrariness: "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. However, recent studies  emphasizing specific ways in which language is non-arbitrary [@MonaghanEtAl14_ArbitraryLanguage; @DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity; @BlasiEtAl16_SoundMeaningAssociation] force us to consider exactly in what sense arbitrariness is an essential property of conventions. In our model, we move to a more probabilistic notion of arbitrariness. The initial speaker distribution may not be perfectly uniform, but different values will nonetheless be sampled in different games. Because the result of this initial choice leads the speaker to update their beliefs about the lexicon in a path-dependent trajectory, this utterance becomes increasingly probable over future rounds. When observing a large number of games, as we did in our experimental analyses, we can therefore still observe substantial variability.

### What does our model say about common ground? 
It's situated somewhere between the non-representational agents of @Barr2004_ConventionalCommunicationSystems and the explicit representation of a common ground memory store used by Brown-Schmidt (XXXX). Agents coordinate on conventions by updating their beliefs based on the same objective history of observations. Still, our model will not be able to account for some empirical effects, such as the impact of explicit listener feedback on the formation of efficient conventions, without relaxing the assumption that one's partner knows the true lexicon with complete certainty. Listeners often reply with question or paraphrases or suggestions for better names. This will require our RSA model to deal with extended dialogues.

### Discuss whether this model generalizes outside language?
While our model was explicitly designed to account for a language phenomenon, many behavioral conventions share the same properties. For example, the real-time coordination games used in @HawkinsGoldstone16_SocialConventions may not require players to reason about a structured lexicon with noise, but an action policy representation may play a similar role. Shared conventions also allow communities of speakers to coordinate group behavior (cite collective behavior lit??) and align beliefs or memories [@StolkVerhagenToni16_ConceptualAlignment; @ComanEtAl16_MnemonicConvergence]. 

### Describe ways of enriching our basic model?
Describing vs. naming? [@Carroll80_NamingHedges] Utterances can be corrupted during transmission; listeners invert this noise model to infer likely intended utterances from their noisy percepts, and speakers take this source of noise into account when selecting what utterance to use [@GibsonBergenPiantadosi13_RationalIntegrationNoisy;@BergenGoodman15_StrategicUseOfNoise]



### Make brief case against other models?
relatively little modeling work has focused on the cognitive mechanisms supporting emergence of local conventions during shorter dyadic interactions. In fact, the convergence on semi-arbitrary yet efficient referring expressions within an interaction poses some problems for these classes of models. Pure evolutionary models do not provide a mechanism for agents to adapt their signaling strategy within a lifetime, let alone within an extended interaction. Emergence-through-use models, such as the one proposed by @Barr2004_ConventionalCommunicationSystems, use simple updating rules based on success or failure in an interaction, but are not easily extended to richer language models and cannot straightforwardly explain the sharp reduction in word length across a repeated interaction with a single partner \textcolor{red}{rdh: might want to actually show this if I'm going to make that argument...}

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
