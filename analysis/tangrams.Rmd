---
title: "tangramsReference"
output: html_document
---

# Import data

```{r}
library(ggplot2)
library(lmer)
library(lmerTest)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(tm)

setwd("~/Repos/reference_games/analysis")

tangramMsgs = read.csv("../data/tangrams/message/tangramsMessages.csv") %>%
  rename(msgTime = time, 
         role = sender)

tangramSubjInfo = read.csv("../data/tangrams/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  filter(!is.na(nativeEnglish)) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

mean(tangramSubjInfo$totalLength / 1000 / 60)
```

```{r}
tangramCombined <- tangramMsgs %>%
  left_join(tangramSubjInfo, by = c("gameid", "role")) %>%
  filter(nativeEnglish != "no") %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

length(unique(tangramCombined$gameid))
```

```{r}
pdf("tangramsFigs/wordOverTime.pdf")
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words (by director) per figure") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw() 
dev.off()
```

# Make wordclouds for round 6

```{r}
library(wordcloud)   

oldGrams = read.csv("oldTangrams.csv", quote = '"') %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

textPerGram = oldGrams %>%
  group_by(gameid, tangram) %>%
  filter(tangram != 0) %>%
  filter(roundNum == 6) %>%
  summarize(a = paste(cleanMsg, collapse = " ")) %>%
  group_by(tangram) %>%
  summarize(text = paste(a, collapse = " ")) %>%
  rename(docs = tangram) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  # pdf(paste("wordcloudForTangram", i, ".pdf", sep = ""))
  freq <- sort(colSums(as.matrix(dtm["1",])), decreasing=TRUE)
  print(entropy(freq))
#   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
#   dev.off()
}
```

# Compute across-pair entropy and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  group_by(gameid, tangram) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  group_by(tangram, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangram") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangram)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```

# Make POS plots... 

```{r}
d = read.csv('~/Repos/tangrams_replication/ipython_notebooks/posTagged.csv', header =T) %>%
  filter(sender == "director") %>%
  group_by(roundNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(NNnum)/sum(numWords),
            verbs = sum(VBnum)/sum(numWords),
            dets= sum(DTnum)/sum(numWords),
            pronouns = sum(PRPnum)/sum(numWords),
            preps = sum(INnum)/sum(numWords),
            tos = sum(TOnum)/sum(numWords),
            adverbs = sum(RBnum)/sum(numWords)) %>%
  mutate(OTHER = (1 - nouns - verbs - dets - pronouns -
                      preps - tos - adverbs)) %>%
  gather(POS, prop, nouns:OTHER) %>%
  select(roundNum, POS, prop) 
  
head(d)

ggplot(d, aes(x = roundNum, y = prop, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw()
```

Same thing but make them add up to number of words... 

```{r}
d = read.csv('~/Repos/tangrams_replication/ipython_notebooks/posTagged.csv', header =T) %>%
  filter(sender == "director") %>%
  group_by(roundNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(NNnum),
            verbs = sum(VBnum),
            dets= sum(DTnum),
            pronouns = sum(PRPnum),
            preps = sum(INnum),
            tos = sum(TOnum),
            adverbs = sum(RBnum)) %>%
  mutate(OTHER = (numWords - nouns - verbs - dets - pronouns -
                      preps - tos - adverbs)) %>%
  gather(POS, total, nouns:OTHER) %>%
  select(roundNum, POS, total) 
head(d)
ggplot(d, aes(x = roundNum, y = total, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw()

```

# Copied from nicki's ipython notebook... Need to recreate this myself

```{r}
nullDistributionNums = c(0.2013888888888889,
 0.19444444444444445,
 0.1423611111111111,
 0.1597222222222222,
 0.13541666666666666,
 0.1527777777777778,
 0.13194444444444445,
 0.17708333333333334,
 0.1388888888888889,
 0.1597222222222222,
 0.1597222222222222,
 0.16666666666666666,
 0.1423611111111111,
 0.21180555555555555,
 0.1527777777777778,
 0.16319444444444445,
 0.1840277777777778,
 0.1423611111111111,
 0.18055555555555555,
 0.15625,
 0.17708333333333334,
 0.1701388888888889,
 0.19444444444444445,
 0.1701388888888889,
 0.16666666666666666,
 0.1701388888888889,
 0.1597222222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1388888888888889,
 0.2048611111111111,
 0.16666666666666666,
 0.14583333333333334,
 0.1736111111111111,
 0.16666666666666666,
 0.16666666666666666,
 0.16666666666666666,
 0.15625,
 0.1388888888888889,
 0.1423611111111111,
 0.1527777777777778,
 0.1597222222222222,
 0.2048611111111111,
 0.15625,
 0.15625,
 0.1597222222222222,
 0.1597222222222222,
 0.13541666666666666,
 0.1527777777777778,
 0.16319444444444445,
 0.16666666666666666,
 0.1423611111111111,
 0.16666666666666666,
 0.1736111111111111,
 0.1388888888888889,
 0.18055555555555555,
 0.14930555555555555,
 0.15625,
 0.1701388888888889,
 0.14583333333333334,
 0.15625,
 0.18055555555555555,
 0.1597222222222222,
 0.1597222222222222,
 0.17708333333333334,
 0.13541666666666666,
 0.14583333333333334,
 0.1284722222222222,
 0.1527777777777778,
 0.2048611111111111, 
 0.1527777777777778,
 0.1527777777777778,
 0.1527777777777778,
 0.1423611111111111,
 0.1597222222222222,
 0.1388888888888889,
 0.1701388888888889,
 0.1597222222222222,
 0.1597222222222222,
 0.17708333333333334,
 0.16319444444444445,
 0.14930555555555555,
 0.1597222222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1423611111111111,
 0.15625,
 0.1736111111111111,
 0.1597222222222222,
 0.1701388888888889,
 0.1527777777777778,
 0.1423611111111111,
 0.15625,
 0.1388888888888889,
 0.19791666666666666,
 0.14583333333333334,
 0.17708333333333334,
 0.1388888888888889,
 0.1909722222222222,
 0.14583333333333334,
 0.1875,
 0.16666666666666666,
 0.19444444444444445,
 0.16319444444444445,
 0.15625,
 0.1527777777777778,
 0.1701388888888889,
 0.12152777777777778,
 0.1701388888888889,
 0.13194444444444445,
 0.1597222222222222,
 0.1388888888888889,
 0.1736111111111111,
 0.1527777777777778,
 0.14930555555555555,
 0.16319444444444445,
 0.1527777777777778,
 0.14583333333333334,
 0.13541666666666666,
 0.1875,
 0.1597222222222222,
 0.16666666666666666,
 0.1909722222222222,
 0.1597222222222222,
 0.16319444444444445,
 0.14930555555555555,
 0.1909722222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1527777777777778,
 0.1736111111111111,
 0.18055555555555555,
 0.1423611111111111,
 0.15625,
 0.1840277777777778,
 0.1388888888888889,
 0.16319444444444445,
 0.14930555555555555,
 0.16319444444444445,
 0.16319444444444445,
 0.1840277777777778,
 0.19791666666666666,
 0.1597222222222222,
 0.1388888888888889,
 0.1736111111111111,
 0.1423611111111111,
 0.13541666666666666,
 0.15625,
 0.1875,
 0.1597222222222222,
 0.16319444444444445,
 0.1597222222222222,
 0.1423611111111111,
 0.1597222222222222,
 0.1875,
 0.13194444444444445,
 0.16319444444444445,
 0.1423611111111111,
 0.16666666666666666,
 0.1388888888888889,
 0.14930555555555555,
 0.1527777777777778,
 0.16319444444444445,
 0.1423611111111111,
 0.1701388888888889,
 0.1388888888888889,
 0.15625,
 0.17708333333333334,
 0.19444444444444445,
 0.1736111111111111,
 0.1701388888888889,
 0.14583333333333334,
 0.16319444444444445,
 0.17708333333333334,
 0.14583333333333334,
 0.1875,
 0.16666666666666666,
 0.17708333333333334,
 0.16666666666666666,
 0.1527777777777778,
 0.1736111111111111,
 0.16666666666666666,
 0.11805555555555555,
 0.18055555555555555,
 0.16666666666666666,
 0.1701388888888889,
 0.1388888888888889,
 0.16666666666666666,
 0.14930555555555555,
 0.1909722222222222,
 0.17708333333333334,
 0.125,
 0.15625,
 0.1701388888888889,
 0.14930555555555555,
 0.11805555555555555,
 0.1597222222222222,
 0.14583333333333334,
 0.16666666666666666,
 0.16319444444444445)

pdf("tangramsFigs/nullDistribution.pdf")
qplot(nullDistributionNums,binwidth = .01) +
  geom_vline(x = 0.2112, color = 'red', size = 4) +
  xlab("probability of match b/w Rounds 1 & 6") +
  ggtitle("Are high PMI words more likely to \n conventionalize than expected by chance?") +
  theme_bw()
dev.off()
```